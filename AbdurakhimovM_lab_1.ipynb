{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSimple07/ArchitecturesNN_for_DL/blob/main/AbdurakhimovM_lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7346a3f-f59b-4eff-8c90-b74dfa802144",
      "metadata": {
        "id": "d7346a3f-f59b-4eff-8c90-b74dfa802144"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet transformers==4.37.2 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2 datasets==2.14.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c66d4c-72a0-41d3-83a2-61972eed62de",
      "metadata": {
        "id": "41c66d4c-72a0-41d3-83a2-61972eed62de"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm.auto import tqdm, trange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import peft\n",
        "\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "import random\n",
        "const_seed = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cef2fb8-c51b-4920-ba2f-b5958da1ee33",
      "metadata": {
        "id": "0cef2fb8-c51b-4920-ba2f-b5958da1ee33"
      },
      "outputs": [],
      "source": [
        "assert torch.cuda.is_available(), \"check out cuda availability (change runtime type in colab)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee51942-6592-4793-a6af-424a4ad5fdee",
      "metadata": {
        "id": "9ee51942-6592-4793-a6af-424a4ad5fdee"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a22fd65-4da8-4e1c-afb3-ac217655e38b",
      "metadata": {
        "id": "9a22fd65-4da8-4e1c-afb3-ac217655e38b"
      },
      "source": [
        "# Part 0: Initializing the model and tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696bc9f8-24e8-4285-b006-8854163159eb",
      "metadata": {
        "id": "696bc9f8-24e8-4285-b006-8854163159eb"
      },
      "source": [
        "let's take mistral model for our experiments (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) that was tuned to follow user instructions. Pay attention that we load model in 4 bit to decrease the memory usage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3864e526-46c3-453f-83cf-b0a1224e49e1",
      "metadata": {
        "id": "3864e526-46c3-453f-83cf-b0a1224e49e1"
      },
      "source": [
        "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e011ad-f5ca-4b84-8d2a-828a50d55d65",
      "metadata": {
        "id": "b8e011ad-f5ca-4b84-8d2a-828a50d55d65"
      },
      "outputs": [],
      "source": [
        "# load llama tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Note: to speed up inference you can use flash attention 2 (https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    load_in_4bit=True, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
        "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
        "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7626dfe0-d5bb-401c-a64f-280360b231d8",
      "metadata": {
        "id": "7626dfe0-d5bb-401c-a64f-280360b231d8"
      },
      "source": [
        "# Part 1 (5 points): Prompt-engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b97541-6f9d-4b97-8c60-746fd7384cb1",
      "metadata": {
        "id": "d4b97541-6f9d-4b97-8c60-746fd7384cb1"
      },
      "source": [
        "**There are different strategies for text generation in huggingface:**\n",
        "\n",
        "| Strategy | Description | Pros & Cons |\n",
        "| --- | --- | --- |\n",
        "| Greedy Search | Chooses the word with the highest probability as the next word in the sequence. | **Pros:** Simple and fast. <br> **Cons:** Can lead to repetitive and incoherent text. |\n",
        "| Sampling with Temperature | Introduces randomness in the word selection. A higher temperature leads to more randomness. | **Pros:** Allows exploration and diverse output. <br> **Cons:** Higher temperatures can lead to nonsensical outputs. |\n",
        "| Nucleus Sampling (Top-p Sampling) | Selects the next word from a truncated vocabulary, the \"nucleus\" of words that have a cumulative probability exceeding a pre-specified threshold (p). | **Pros:** Balances diversity and quality. <br> **Cons:** Setting an optimal 'p' can be tricky. |\n",
        "| Beam Search | Explores multiple hypotheses (sequences of words) at each step, and keeps the 'k' most likely, where 'k' is the beam width. | **Pros:** Produces more reliable results than greedy search. <br> **Cons:** Can lack diversity and lead to generic responses. |\n",
        "| Top-k Sampling | Randomly selects the next word from the top 'k' words with the highest probabilities. | **Pros:** Introduces randomness, increasing output diversity. <br> **Cons:** Random selection can sometimes lead to less coherent outputs. |\n",
        "| Length Normalization | Prevents the model from favoring shorter sequences by dividing the log probabilities by the sequence length raised to some power. | **Pros:** Makes longer and potentially more informative sequences more likely. <br> **Cons:** Tuning the normalization factor can be difficult. |\n",
        "| Stochastic Beam Search | Introduces randomness into the selection process of the 'k' hypotheses in beam search. | **Pros:** Increases diversity in the generated text. <br> **Cons:** The trade-off between diversity and quality can be tricky to manage. |\n",
        "| Decoding with Minimum Bayes Risk (MBR) | Chooses the hypothesis (out of many) that minimizes expected loss under a loss function. | **Pros:** Optimizes the output according to a specific loss function. <br> **Cons:** Computationally more complex and requires a good loss function. |\n",
        "\n",
        "Documentation references:\n",
        "- [reference for `AutoModelForCausalLM.generate()`](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "- [reference for `AutoTokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)\n",
        "- Huggingface [docs on generation strategies](https://huggingface.co/docs/transformers/generation_strategies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3158ac83-2927-4e25-a478-7a6f8931b4b6",
      "metadata": {
        "id": "3158ac83-2927-4e25-a478-7a6f8931b4b6"
      },
      "outputs": [],
      "source": [
        "# TODO: create a function for generation with huggingface\n",
        "def get_answer(tokenizer, model, messages, max_new_tokens=200,\n",
        "               temperature=0.5, do_sample=True):\n",
        "    # TODO: tokenize input, generate answer and decode output. Pay attention to tokenizer methods\n",
        "\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d87944-135e-4696-89a3-62133da6d466",
      "metadata": {
        "id": "52d87944-135e-4696-89a3-62133da6d466"
      },
      "outputs": [],
      "source": [
        "# Let's try our model\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write an explanation of tensors for 5 year old\"},\n",
        "]\n",
        "\n",
        "print(get_answer(tokenizer, model, messages)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ecd955-bc33-4da9-8eb2-df13945f0839",
      "metadata": {
        "id": "90ecd955-bc33-4da9-8eb2-df13945f0839"
      },
      "source": [
        "You should obtain an explanation from the model. If so, let us go further!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e061404b-41f8-482f-b2a8-8d1c6f139f28",
      "metadata": {
        "id": "e061404b-41f8-482f-b2a8-8d1c6f139f28"
      },
      "source": [
        "Now we will take a sample from boolQ (https://huggingface.co/datasets/google/boolq) dataset and try prompting techniques to extract the needed answer and calculate its quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba03448-6320-4dc6-8b27-5cac64207bcd",
      "metadata": {
        "scrolled": true,
        "id": "1ba03448-6320-4dc6-8b27-5cac64207bcd"
      },
      "outputs": [],
      "source": [
        "df = load_dataset(\"google/boolq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0363281f-a172-469c-a12a-63568e031d79",
      "metadata": {
        "id": "0363281f-a172-469c-a12a-63568e031d79"
      },
      "outputs": [],
      "source": [
        "# Fixing 20 validation examples\n",
        "\n",
        "random.seed(const_seed)\n",
        "idx = random.sample(range(1, 3270), 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9eb6d89-1627-4875-8aa6-6da57e137246",
      "metadata": {
        "id": "c9eb6d89-1627-4875-8aa6-6da57e137246"
      },
      "outputs": [],
      "source": [
        "# sample you will work with\n",
        "df_sample = df[\"validation\"].select(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec41c3eb-d827-4e38-b622-3d21cb540d8c",
      "metadata": {
        "id": "ec41c3eb-d827-4e38-b622-3d21cb540d8c"
      },
      "outputs": [],
      "source": [
        "# For instance, you can construct your prompt the following way\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": '''You are given a text and question. Answer only \"true\" or \"false\".\n",
        "text: As with other games in The Elder Scrolls series, the game is set on the continent of Tamriel. The events of the game occur a millennium before those of The Elder Scrolls V: Skyrim and around 800 years before The Elder Scrolls III: Morrowind and The Elder Scrolls IV: Oblivion. It has a broadly similar structure to Skyrim, with two separate conflicts progressing at the same time, one with the fate of the world in the balance, and one where the prize is supreme power on Tamriel. In The Elder Scrolls Online, the first struggle is against the Daedric Prince Molag Bal, who is attempting to meld the plane of Mundus with his realm of Coldharbour, and the second is to capture the vacant imperial throne, contested by three alliances of the mortal races. The player character has been sacrificed to Molag Bal, and Molag Bal has stolen their soul, the recovery of which is the primary game objective.\n",
        "question: is elder scrolls online the same as skyrim\n",
        "answer: '''},\n",
        "]\n",
        "\n",
        "print(get_answer(tokenizer, model, messages)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4183b1f-7a62-4d2f-8227-e52cb5d37d84",
      "metadata": {
        "id": "e4183b1f-7a62-4d2f-8227-e52cb5d37d84"
      },
      "source": [
        "Is anything wrong with the output? Now it is time for you to play around and try to come up with some better prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b00938-a04c-4924-9a28-9041ead50817",
      "metadata": {
        "id": "62b00938-a04c-4924-9a28-9041ead50817"
      },
      "outputs": [],
      "source": [
        "# TODO: create function to evaluate answers\n",
        "# Note: you can adapt function for different answer structures,\n",
        "# but you should be able to automatically extract the target \"true\" or \"false\" components\n",
        "def evaluate_answers(true_answers, predictions):\n",
        "    #\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd21f1b7-1fc5-47ad-b14f-85f3644847f0",
      "metadata": {
        "id": "bd21f1b7-1fc5-47ad-b14f-85f3644847f0"
      },
      "source": [
        "TODO: Try and compare \"naive\" prompting (your best hand-crafted variant), few-shot prompting (https://www.promptingguide.ai/techniques/fewshot) and chain-of-thought prompting (step-be-step thinking - https://www.promptingguide.ai/techniques/cot).\n",
        "\n",
        "Save the generation results into separate csv files and do not forget to attach them to your homework."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e446cfb-7e1f-446e-966c-aa6f6725eddf",
      "metadata": {
        "id": "9e446cfb-7e1f-446e-966c-aa6f6725eddf"
      },
      "source": [
        "# Part 2 (5 points): Fine-tuning with PEFT and LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c7e149-4833-46cd-9431-882ba6e5f83d",
      "metadata": {
        "id": "f5c7e149-4833-46cd-9431-882ba6e5f83d"
      },
      "outputs": [],
      "source": [
        "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM,\n",
        "                                      num_virtual_tokens=16) #\n",
        "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-plac)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b91921-3a2a-4e68-9202-19fb1b792047",
      "metadata": {
        "id": "56b91921-3a2a-4e68-9202-19fb1b792047"
      },
      "outputs": [],
      "source": [
        "model.print_trainable_parameters() # Wow so small amount of trainable params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34f6fdae-0961-422c-97a8-04f50d2387bc",
      "metadata": {
        "id": "34f6fdae-0961-422c-97a8-04f50d2387bc"
      },
      "outputs": [],
      "source": [
        "# creating simple prompt formating\n",
        "def format_prompt(sample):\n",
        "    return f'''\n",
        "    text: {sample['passage']}\n",
        "    question: {sample['question']}\n",
        "    answer: {sample['answer']}\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425ed2aa-5c31-4f9c-a7b7-da7061a39d89",
      "metadata": {
        "id": "425ed2aa-5c31-4f9c-a7b7-da7061a39d89"
      },
      "source": [
        "TODO: initialize Trainer and pass train part of our dataset for 2-3 epoches\n",
        "\n",
        "Note: carefully set max_seq_length and args (that are transformers.TrainingArguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e02de8d-5401-4d7e-8a82-28cbeeee0dc1",
      "metadata": {
        "id": "6e02de8d-5401-4d7e-8a82-28cbeeee0dc1"
      },
      "source": [
        "TODO: save and check your tuned model. Provide scores on our 20 validation examples and save result to csv file"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}